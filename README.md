## Guanaco-lora: LoRA for trainin Multilingual Instruction-following LM based on LLaMA

- ðŸ¤— **Try the pretrained model out [here](https://huggingface.co/KBlueLeaf/guanaco-7b-leh-v2)**

This repository is forked from alpaca-lora, and introduce a method to train more modules like embed/head with lora.

with trained embed and head, you can get better result on multilang performance.

## Dataset

We use cleaned version alpaca from alpaca-lora and whole guanaco dataset to train the pretrained model.

guanaco dataset: [link](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)


## Usage

basically as same as alpaca lora

## Example
Some example of instruction-based QA and instruction-based chat
![image](https://user-images.githubusercontent.com/59680068/227710520-b24a6a88-5982-43eb-b3ac-e1b371dc0082.png)

## Resource
Todo...
